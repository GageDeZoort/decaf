[I 20:50:58.154 NotebookApp] Serving notebooks from local directory: /home/matteoc/decaf
[I 20:50:58.155 NotebookApp] The Jupyter Notebook is running at:
[I 20:50:58.155 NotebookApp] http://127.0.0.1:9094/?token=f826d5840d523c089db94cbf5d741eacb0bb4a1ff5f37b5b
[I 20:50:58.155 NotebookApp]  or http://127.0.0.1:9094/?token=f826d5840d523c089db94cbf5d741eacb0bb4a1ff5f37b5b
[I 20:50:58.155 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 20:50:58.192 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///home/matteoc/.local/share/jupyter/runtime/nbserver-35051-open.html
    Or copy and paste one of these URLs:
        http://127.0.0.1:9094/?token=f826d5840d523c089db94cbf5d741eacb0bb4a1ff5f37b5b
     or http://127.0.0.1:9094/?token=f826d5840d523c089db94cbf5d741eacb0bb4a1ff5f37b5b
[I 20:51:12.564 NotebookApp] 302 GET /?token=f826d5840d523c089db94cbf5d741eacb0bb4a1ff5f37b5b (127.0.0.1) 0.81ms
[I 20:51:20.537 NotebookApp] Kernel started: dfa4ad0d-01bc-49d5-b745-fb08121547c0
Ivy Default Cache set to: /home/matteoc/.ivy2/cache
The jars for the packages stored in: /home/matteoc/.ivy2/jars
:: loading settings :: url = jar:file:/opt/spark-distro/spark-2.4.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
edu.vanderbilt.accre#laurelin added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-eb9d188f-109b-4e0b-aae4-0187632372d5;1.0
	confs: [default]
	found edu.vanderbilt.accre#laurelin;0.4.2-SNAPSHOT in local-m2-cache
	found com.google.guava#guava;28.0-jre in central
	found com.google.guava#failureaccess;1.0.1 in central
	found com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central
	found com.google.code.findbugs#jsr305;3.0.2 in central
	found org.checkerframework#checker-qual;2.8.1 in central
	found com.google.errorprone#error_prone_annotations;2.3.2 in central
	found com.google.j2objc#j2objc-annotations;1.3 in central
	found org.codehaus.mojo#animal-sniffer-annotations;1.17 in central
	found org.tukaani#xz;1.2 in central
	found org.lz4#lz4-java;1.5.1 in central
	found org.apache.logging.log4j#log4j-api;2.11.2 in central
	found org.apache.logging.log4j#log4j-core;2.11.2 in central
:: resolution report :: resolve 1047ms :: artifacts dl 157ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
	com.google.errorprone#error_prone_annotations;2.3.2 from central in [default]
	com.google.guava#failureaccess;1.0.1 from central in [default]
	com.google.guava#guava;28.0-jre from central in [default]
	com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]
	com.google.j2objc#j2objc-annotations;1.3 from central in [default]
	edu.vanderbilt.accre#laurelin;0.4.2-SNAPSHOT from local-m2-cache in [default]
	org.apache.logging.log4j#log4j-api;2.11.2 from central in [default]
	org.apache.logging.log4j#log4j-core;2.11.2 from central in [default]
	org.checkerframework#checker-qual;2.8.1 from central in [default]
	org.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]
	org.lz4#lz4-java;1.5.1 from central in [default]
	org.tukaani#xz;1.2 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-eb9d188f-109b-4e0b-aae4-0187632372d5
	confs: [default]
	0 artifacts copied, 13 already retrieved (0kB/42ms)
19/11/11 20:51:33 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
19/11/11 20:51:34 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore
19/11/11 20:51:34 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore
19/11/11 20:51:34 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
19/11/11 20:51:35 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
19/11/11 20:51:56 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
[I 20:53:20.444 NotebookApp] Saving file at /analysis/run_spark.ipynb
